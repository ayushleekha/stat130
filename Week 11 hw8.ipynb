{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012bcd9c",
   "metadata": {},
   "source": [
    "1)\n",
    "A Classification Decision Tree is a supervised learning algorithm used to solve classification problems. These are problems where the goal is to predict a discrete category (or class) for an input based on its features. The decision tree models the problem as a series of decisions, represented by a tree structure, where each node splits the data based on a specific feature condition, leading to leaf nodes that represent the predicted class.\n",
    "\n",
    "Examples of real-world applications:\n",
    "\n",
    "Medical Diagnosis: Predicting whether a patient has a specific disease based on symptoms and test results.\n",
    "Spam Filtering: Classifying emails as \"Spam\" or \"Not Spam.\"\n",
    "Credit Risk Assessment: Determining whether a loan applicant is \"High Risk\" or \"Low Risk\" based on financial and demographic features.\n",
    "Customer Segmentation: Categorizing customers into groups (e.g., \"Frequent Shoppers\" or \"Occasional Shoppers\") for targeted marketing.\n",
    "Fraud Detection: Identifying fraudulent transactions based on patterns in transaction data.\n",
    "(b) Comparison: Classification Decision Tree vs. Multiple Linear Regression\n",
    "How a Classification Decision Tree Makes Predictions:\n",
    "A decision tree works by splitting the dataset into smaller subsets based on decision rules that optimize a criterion like the Gini impurity or entropy (in classification tasks).\n",
    "Each split corresponds to a condition on a feature, and the process continues until a stopping condition is met (e.g., all samples in a node belong to the same class).\n",
    "For a new input, the decision tree follows the rules down the tree until it reaches a leaf node, which provides the class prediction.\n",
    "Example:\n",
    "\n",
    "Feature: \"Annual Income\"\n",
    "Split: \"Is Annual Income > $50,000?\" → Follow one branch if \"Yes,\" another if \"No.\"\n",
    "How Multiple Linear Regression Makes Predictions:\n",
    "Multiple linear regression models a continuous target variable by finding the best-fit linear relationship between the input features and the target.\n",
    "The model predicts the target as a weighted sum of the input features plus a bias term.\n",
    "Equation:  y = β0 + β1x1 + β2x2 +…+ βnxn where y is the predicted output,  x1 , x2 , … , xn are features, and  β0, β1, … , βn are learned coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9802657",
   "metadata": {},
   "source": [
    "2)\n",
    "1. Accuracy\n",
    "\n",
    "Definition:\n",
    "Measures the proportion of all correct predictions (true positives and true negatives) out of the total predictions.\n",
    "Best Scenario:\n",
    "Accuracy is most appropriate in scenarios where the classes are balanced and all errors are equally important. For example:\n",
    "\n",
    "Image Classification in Autonomous Vehicles: Classifying road signs like \"Stop,\" \"Yield,\" and \"Speed Limit.\" Misclassifying these has similar consequences, and the dataset is likely well-balanced.\n",
    "Rationale:\n",
    "When the dataset has roughly equal numbers of classes, and false positives and false negatives carry similar weight, accuracy gives a clear view of overall model performance.\n",
    "\n",
    "2. Sensitivity (Recall)\n",
    "\n",
    "Definition:\n",
    "Measures the proportion of actual positives that are correctly predicted.\n",
    "Best Scenario:\n",
    "Sensitivity is critical in situations where missing positives (false negatives) has severe consequences. For example:\n",
    "\n",
    "Medical Diagnostics for Rare Diseases: Detecting cancer or other life-threatening conditions. Missing a case (false negative) can be life-threatening.\n",
    "Rationale:\n",
    "High sensitivity ensures that most positive cases are identified, minimizing false negatives even if it means tolerating more false positives.\n",
    "\n",
    "3. Specificity\n",
    "\n",
    "Definition:\n",
    "Measures the proportion of actual negatives that are correctly predicted.\n",
    "Best Scenario:\n",
    "Specificity is key when avoiding false positives is more critical than catching every positive. For example:\n",
    "\n",
    "Legal Applications in Criminal Justice: Screening candidates for parole eligibility. Incorrectly labeling a safe candidate as dangerous (false positive) could have societal consequences.\n",
    "Rationale:\n",
    "High specificity ensures that most negatives are correctly identified, minimizing false positives, which can lead to unnecessary actions or mistrust in the system.\n",
    "\n",
    "4. Precision\n",
    "\n",
    "Definition:\n",
    "Measures the proportion of predicted positives that are actually correct.\n",
    "Best Scenario:\n",
    "Precision is crucial in scenarios where false positives are particularly costly or harmful. For example:\n",
    "\n",
    "Spam Email Detection: Identifying spam emails. Flagging legitimate emails as spam (false positives) can lead to important communications being missed.\n",
    "Rationale:\n",
    "High precision ensures that when the model predicts a positive (e.g., spam), it’s almost always correct, reducing the inconvenience of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc1925",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the provided URL\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Subset the data to exclude the specified columns\n",
    "columns_to_keep = [col for col in ab.columns if col not in [\"Weight_oz\", \"Width\", \"Height\"]]\n",
    "ab_reduced = ab[columns_to_keep]\n",
    "\n",
    "# Drop rows with NaN entries in the remaining columns\n",
    "ab_reduced_noNaN = ab_reduced.dropna()\n",
    "\n",
    "# Set data types as specified\n",
    "ab_reduced_noNaN[\"Pub year\"] = ab_reduced_noNaN[\"Pub year\"].astype(int)\n",
    "ab_reduced_noNaN[\"NumPages\"] = ab_reduced_noNaN[\"NumPages\"].astype(int)\n",
    "ab_reduced_noNaN[\"Hard_or_Paper\"] = ab_reduced_noNaN[\"Hard_or_Paper\"].astype(\"category\")\n",
    "\n",
    "# Display basic summary of the processed dataset\n",
    "ab_reduced_noNaN.info(), ab_reduced_noNaN.describe(), ab_reduced_noNaN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f9f91",
   "metadata": {},
   "source": [
    "4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f4062e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets (80/20 split)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ab_reduced_noNaN_train, ab_reduced_noNaN_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mab_reduced_noNaN\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Report the number of observations in the training and testing sets\u001b[39;00m\n\u001b[1;32m     11\u001b[0m train_size \u001b[38;5;241m=\u001b[39m ab_reduced_noNaN_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20 split)\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(\n",
    "    ab_reduced_noNaN, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Report the number of observations in the training and testing sets\n",
    "train_size = ab_reduced_noNaN_train.shape[0]\n",
    "test_size = ab_reduced_noNaN_test.shape[0]\n",
    "\n",
    "# Define the target variable (y) and feature (X)\n",
    "y = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])[\"H\"]\n",
    "X = ab_reduced_noNaN_train[[\"List Price\"]]\n",
    "\n",
    "# Train a DecisionTreeClassifier with max_depth=2\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the fitted decision tree\n",
    "tree_plot = tree.plot_tree(clf, feature_names=[\"List Price\"], class_names=[\"Paperback\", \"Hardcover\"], filled=True)\n",
    "\n",
    "train_size, test_size, clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936560f2",
   "metadata": {},
   "source": [
    "5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a12064",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the new feature set (X) for the second model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X2 \u001b[38;5;241m=\u001b[39m \u001b[43mab_reduced_noNaN_train\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumPages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThick\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train a DecisionTreeClassifier with max_depth=4\u001b[39;00m\n\u001b[1;32m      5\u001b[0m clf2 \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the new feature set (X) for the second model\n",
    "X2 = ab_reduced_noNaN_train[[\"NumPages\", \"Thick\", \"List Price\"]]\n",
    "\n",
    "# Train a DecisionTreeClassifier with max_depth=4\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X2, y)\n",
    "\n",
    "# Plot the fitted decision tree for clf2\n",
    "tree_plot2 = tree.plot_tree(\n",
    "    clf2,\n",
    "    feature_names=[\"NumPages\", \"Thick\", \"List Price\"],\n",
    "    class_names=[\"Paperback\", \"Hardcover\"],\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a94cee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reload the dataset and preprocess\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m ab \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Subset to exclude specified columns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m columns_to_keep \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m ab\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight_oz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Reload the dataset and preprocess\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Subset to exclude specified columns\n",
    "columns_to_keep = [col for col in ab.columns if col not in [\"Weight_oz\", \"Width\", \"Height\"]]\n",
    "ab_reduced = ab[columns_to_keep]\n",
    "\n",
    "# Drop rows with NaN entries\n",
    "ab_reduced_noNaN = ab_reduced.dropna()\n",
    "\n",
    "# Convert data types as specified\n",
    "ab_reduced_noNaN[\"Pub year\"] = ab_reduced_noNaN[\"Pub year\"].astype(int)\n",
    "ab_reduced_noNaN[\"NumPages\"] = ab_reduced_noNaN[\"NumPages\"].astype(int)\n",
    "ab_reduced_noNaN[\"Hard_or_Paper\"] = ab_reduced_noNaN[\"Hard_or_Paper\"].astype(\"category\")\n",
    "\n",
    "# Perform the 80/20 train-test split\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(\n",
    "    ab_reduced_noNaN, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the target and features for the second model\n",
    "y2 = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])[\"H\"]\n",
    "X2 = ab_reduced_noNaN_train[[\"NumPages\", \"Thick\", \"List Price\"]]\n",
    "\n",
    "# Train a DecisionTreeClassifier with max_depth=4\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X2, y2)\n",
    "\n",
    "# Plot the fitted decision tree for clf2\n",
    "tree_plot2 = tree.plot_tree(\n",
    "    clf2,\n",
    "    feature_names=[\"NumPages\", \"Thick\", \"List Price\"],\n",
    "    class_names=[\"Paperback\", \"Hardcover\"],\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "ab_reduced_noNaN_train.shape[0], ab_reduced_noNaN_test.shape[0], clf2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ced2fb",
   "metadata": {},
   "source": [
    "6)\n",
    "1.Key Concepts\n",
    "Positive (P): In our case, a \"Hardcover\" book is treated as the positive class.\n",
    "Negative (N): A \"Paperback\" book is the negative class.\n",
    "True Positive (TP): A hardcover book correctly predicted as hardcover.\n",
    "True Negative (TN): A paperback book correctly predicted as paperback.\n",
    "False Positive (FP): A paperback book incorrectly predicted as hardcover.\n",
    "False Negative (FN): A hardcover book incorrectly predicted as paperback.\n",
    "Confusion Matrix in sklearn:\n",
    "Rows represent the true labels (y_true).\n",
    "Columns represent the predicted labels (y_pred).\n",
    "Order of Arguments in confusion_matrix:\n",
    "confusion_matrix(y_true, y_pred) → True labels first, predicted labels second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60b2dc",
   "metadata": {},
   "source": [
    "7)\n",
    "The differences between the two confusion matrices are primarily caused by the number and type of features used to make predictions. The first confusion matrix uses only the \"List Price\" feature to classify books, which may not provide enough information for accurate classification. In contrast, the second matrix incorporates additional features—\"NumPages\" and \"Thick\"—allowing the model to better capture the relationships between predictors and the target variable.\n",
    "\n",
    "The confusion matrices for the test set (clf and clf2) are better because they evaluate the models on unseen data, providing a more reliable assessment of how well the models generalize to new observations. Evaluating on training data often leads to overly optimistic results, as the model has already been optimized for this data. This distinction highlights the importance of using a separate test set for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d86a40",
   "metadata": {},
   "source": [
    "8)\n",
    "To visualize and interpret feature importances in a scikit-learn Decision Tree, you can use the.feature_importances_ attribute of the trained model. This attribute provides the relative importance of each predictor variable in determining the predictions, calculated based on the reduction in the chosen criterion (e.g., Gini impurity or Shannon entropy) contributed by splits involving that feature.\n",
    "\n",
    "Here’s how you can visualize feature importances and identify the most important predictor for clf2:\n",
    "\n",
    "Steps:\n",
    "Access clf2.feature_importances_: This provides an array of importance scores, one for each feature.\n",
    "Access clf2.feature_names_in_: This lists the names of the features used in the model, matching the order of the importance scores.\n",
    "Visualize Importances: Use a bar plot to display the importance values for each feature, making it easier to interpret.\n",
    "Determine the Most Important Feature: Find the feature with the highest importance score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c198c9d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract feature importances and feature names\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m \u001b[43mclf2\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m      6\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m clf2\u001b[38;5;241m.\u001b[39mfeature_names_in_\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Identify the most important feature\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf2' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract feature importances and feature names\n",
    "feature_importances = clf2.feature_importances_\n",
    "feature_names = clf2.feature_names_in_\n",
    "\n",
    "# Identify the most important feature\n",
    "most_important_feature = feature_names[np.argmax(feature_importances)]\n",
    "\n",
    "# Visualize feature importances as a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(feature_names, feature_importances, color='skyblue')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importances in clf2\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "most_important_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618acc9",
   "metadata": {},
   "source": [
    "In linear regression, the coefficients represent the direct impact of each predictor variable on the outcome, assuming all other variables are held constant. Each coefficient indicates how much the predicted value of the target variable changes with a one-unit change in the predictor, making interpretation straightforward and additive.\n",
    "\n",
    "In contrast, feature importances in decision trees indicate how much each predictor variable contributes to reducing uncertainty (e.g., Gini impurity or entropy) across all splits in the tree. This interpretation is less direct, as it aggregates the contributions of a feature across potentially complex interactions and non-linear relationships throughout the tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
